### 컨테이너 인프라 환경

리눅스 운영체제의 **커널 하나에 여러 개의 컨테이너(프로세스)**가 격리된 상태로 실행

쿠버네티스의 등장으로 효율적으로 컨테이너 인프라 환경 관리가 가능해지면서 생태계가 확장됨

### 쿠버네티스를 왜 사용할까

정확하게 얘기하면 쿠버네티스는 컨테이너 Orchestration을 위한 솔루션으로 다수의 컨테이너를 유기적으로 연결, 실행, 종료, 상태 추적 등을 도와준다. 

다른 솔루션을 제치고 쿠버네티스가 시장을 선점한 이유는 다양한 형태로 활용이 가능하기 때문이다. IT 인프라 자체를 컨테이너화할 수 있다. 

학습 곡선 자체는 어려우나

1. 매우 안정적이고
2. 확장성이 좋으며
3. 세부 설정 지원이 다양하게 있고
4. 정보량도 많고
5. 에코 파트너도 많다.


# 임시노트

### kubeadm으로 쿠버네티스 구성하기

1. Vagrantfile 
    - vagrant up 명령어를 입력하면, vagrantfile을 읽어 정의된 가상 머신들을 생성하고 생성한 가상머신에 쿠버네티스 클러스터를 구성하기 위한 파일을 호출해 자동 구성

```bash
# -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  N = 3 # worker node의 수, args : N이 적힌 곳에서 config.sh로 넘김
  Ver = '1.18.4' # 설치할 쿠버네티스 버전 지정 

  #=============#
  # Master Node #
  #=============#

    config.vm.define "m-k8s" do |cfg|
      cfg.vm.box = "sysnet4admin/CentOS-k8s"
      cfg.vm.provider "virtualbox" do |vb|
        vb.name = "m-k8s(github_SysNet4Admin)"
        vb.cpus = 2
        vb.memory = 3072
        vb.customize ["modifyvm", :id, "--groups", "/k8s-SgMST-1.13.1(github_SysNet4Admin)"]
      end
      cfg.vm.host_name = "m-k8s"
      cfg.vm.network "private_network", ip: "192.168.1.10"
      cfg.vm.network "forwarded_port", guest: 22, host: 60010, auto_correct: true, id: "ssh"
      cfg.vm.synced_folder "../data", "/vagrant", disabled: true 
      cfg.vm.provision "shell", path: "config.sh", args: N
# 버전과 Main을 install_pkg.sh로 넘김. 
# Main문자는 install_pkg.sh에서 조건문으로 처리해 마스터 노드에만 전체 실행 코드를 내려받게함
      cfg.vm.provision "shell", path: "install_pkg.sh", args: [ Ver, "Main" ] 
      cfg.vm.provision "shell", path: "master_node.sh"
    end

  #==============#
  # Worker Nodes #
  #==============#

  (1..N).each do |i|
    config.vm.define "w#{i}-k8s" do |cfg|
      cfg.vm.box = "sysnet4admin/CentOS-k8s"
      cfg.vm.provider "virtualbox" do |vb|
        vb.name = "w#{i}-k8s(github_SysNet4Admin)"
        vb.cpus = 1
        vb.memory = 2560
        vb.customize ["modifyvm", :id, "--groups", "/k8s-SgMST-1.13.1(github_SysNet4Admin)"]
      end
      cfg.vm.host_name = "w#{i}-k8s"
      cfg.vm.network "private_network", ip: "192.168.1.10#{i}"
      cfg.vm.network "forwarded_port", guest: 22, host: "6010#{i}", auto_correct: true, id: "ssh"
      cfg.vm.synced_folder "../data", "/vagrant", disabled: true
      cfg.vm.provision "shell", path: "config.sh", args: N
      cfg.vm.provision "shell", path: "install_pkg.sh", args: Ver
      cfg.vm.provision "shell", path: "work_nodes.sh"
    end
  end

end
```

1. [config.sh](http://config.sh) 
    - kubeadm으로 쿠버네티스 설치를 위한 사전 조건 설정 스크립트 파일

```bash
#!/usr/bin/env bash

# vim config - vi를 입력하면 vim을 호출하게 설정
echo 'alias vi=vim' >> /etc/profile

# 쿠버네티스 설치 조건 상 스왑되면 안된다고 함
swapoff -a
# 시스템이 다시 시작되더라도 스왑되지 않게 설정
sed -i.bak -r 's/(.+ swap .+)/#\1/' /etc/fstab

# 쿠버네티스 레포 설정 경로를 변수로 설정
# 그 아래부턴 쿠버네티스를 내려받을 레포를 설정함
gg_pkg="packages.cloud.google.com/yum/doc" 
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://${gg_pkg}/yum-key.gpg https://${gg_pkg}/rpm-package-key.gpg
EOF

# selinux를 permissive mode로 변경(제한 풀기)
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

# IPv4 Ipv6의 패킷을 iptables가 관리하게 설정, pod의 통신을 iptable로 제어(다른 걸로 구성가능)
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
modprobe br_netfilter # br_netfilter 커널 모듈을 사용해 브리지로 네트워크 구성. 적어줘야 iptable이 활성화됨

# 쿠버네티스 안 노드 간 통신을 이름으로 할 수 있게 각 노드 호스트 이름, ip를 /etc/hosts에 설정
# 위에 N 변수가(노드 수) 여기로 전달됨
echo "192.168.1.10 m-k8s" >> /etc/hosts
for (( i=1; i<=$1; i++  )); do echo "192.168.1.10$i w$i-k8s" >> /etc/hosts; done

# config DNS  
cat <<EOF > /etc/resolv.conf
nameserver 1.1.1.1 #cloudflare DNS
nameserver 8.8.8.8 #Google DNS
EOF
```

1. install_pkg.sh
    - 클러스터 구성을 위해 가상 머신에 설치해야 할 의존성 패키지 명시

```bash
#!/usr/bin/env bash

yum install epel-release -y
yum install vim-enhanced -y
yum install git -y # 깃헙에서 내려받을 수 있게 깃 설정함

# 쿠버네티스를 관리하는 컨테이너 설치를 위해 도커 설치하고 구동
yum install docker -y && systemctl enable --now docker

# 넘겨 받은 Ver 버전의 kubectl, kubelet, kubeadm을 설치하고 kubelet을 시작함
yum install kubectl-$1 kubelet-$1 kubeadm-$1 -y
systemctl enable --now kubelet

# 전체 실행 코드를 마스터 노드만 내려받도록 vagrantfile에서 두 번쨰 변수인 Main을 넘겨받음
# 깃에서 코드를 내려받아 실습을 진행할 루트 홈 디렉터리(/root)로 옮김
# .sh를 find로 찾아 바로 실행 가능한 상태가 되도록 chmod700으로 설정
if [ $2 = 'Main' ]; then
  git clone https://github.com/sysnet4admin/_Book_k8sInfra.git
  mv /home/vagrant/_Book_k8sInfra $HOME
  find $HOME/_Book_k8sInfra/ -regex ".*\.\(sh\)" -exec chmod 700 {} \;
fi
```

1. master_node.sh
    - 1개의 가상 머신을 쿠버네티스 마스터 노드로 구성하는 스크립트
    - 여기선 CNI(쿠버네티스 네트워크 인터페이스)도 함께 구성

```bash
#!/usr/bin/env bash

# kubeadm을 통해 쿠버네티스의 워커 노드를 받을 준비
# 토큰을 123456.~ 으로 지정하고 ttl(유지 시간)을 0으로 설정 -> 기본값인 24시간 후 토큰이 계속 유지되게함
# 워커 노드가 정해진 토큰으로 들어오게함
kubeadm init --token 123456.1234567890123456 --token-ttl 0 \
--pod-network-cidr=172.16.0.0/16 --apiserver-advertise-address=192.168.1.10 

# 마스터 노드에서 현재 사용자가 쿠버네티스를 정상적으로 구동할 수 있게 설정 파일을 홈디렉터리에 복사
# 쿠버네티스를 이용할 사용자에게 권한 설정 
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

# CNI인 Calico의 설정을 적용해 쿠버네티스 네트워크를 구성 
kubectl apply -f \
https://raw.githubusercontent.com/sysnet4admin/IaC/master/manifests/172.16_net_calico.yaml
```

1. work_nodes.sh
    - 쿠버네티스 워커 노드를 구성하는 스크립트
    - 마스터 노드에 구성된 클러스터에 조인이 필요한 정보가 모두 코드화돼 있어 스크립트를 실행하기만 하면 편하게 워커 노드로서 쿠버네티스 클러스터에 조인됨

```bash
#!/usr/bin/env bash

# kubeadm을 이용해 마스터 노드에 접속
# discovery-token-unsafe로 인증을 무시하고 api 서버 주소로 접속 
kubeadm join --token 123456.1234567890123456 \
--discovery-token-unsafe-skip-ca-verification 192.168.1.10:6443
```

### pod 배포 순서에 따른 쿠버네티스 구성 요소

1. kubectl
    1. 쿠버네티스 클러스터에 명령을 내림
    2. 보통 api 서버와 주로 통신해 마스터 노드에 두기도 함(안둬도 상관 없음)(m-k8s가 마스터노드)
2. API 서버
    1. 주로 상태값을 저장하는 etcd와 통신, 그 외 API 서버 중심으로 통신하기 때문에 중심 역할
3. etcd
    1. 구성 요소들의 상태 값이 모두 저장됨. 백업 해두면 후에 그대로 쿠버네티스 클러스터 복구 가능. 
    2. key-value 저장소
4. 컨트롤러 매니저
    
    쿠버네티스 클러스터의 오브젝트 상태를 관리. 상태 체크, 복구 등
    
5. 스케줄러
    
    노드 상태, 자원 등을 고려해 어떤 worker node에 pod을 생성할 지 결정하고 할당
    
6. kubelet
    
    pod의 구성 내용(podSpec)을 컨테이너 런타임으로 전달, 모니터링함
    
7. 컨테이너 런타임(CRI, Container Runtime Interface)
    
    pod을 이루는 컨테이너를 실행하는 표준 인터페이스
    
8. pod
    
    1개 이상의 컨테이너로 일을 하기 위해 모인 단위. 언제라도 죽을 수 있음
    

1~8번은 기본 설정이지만 아래부터는 선택 설정임

1. 네트워크 플러그인
    
    클러스터 통신을 위해 플러그인을 설치. 일반적으로 CNI로 구성하는데 여기선 캘리코를 사용
    
2. CoreDNS
    
    DNS 서버
    

### 사용자가 배포된 pod에 접속할 때

1. kube-proxy
    1. kube-proxy를 이용해 pod이 위치한 노드로 통신할 수 있는 네트워크를 설정함
    2. 실제 통신은 br_netfilter, iptables로 관리함(위의 설정 파일에 있는 내용)
2. pod
    1. 이미 배포된 pod에 접속하고 필요한 내용을 전달받음
    2. 사용자는 pod이 어떤 워커 노드에 있는지 신경 안써도 됨
    

### Pod 생명주기로 쿠버네티스 구성 요소 살펴보기

1. kubectl을 통해 API 서버에 pod 생성 요청
2. (업데이트가 있을 때마다) API 서버에 전달된 내용을 etcd에 기록하고 클러스터 상태값을 최신으로 유지함, etcd도 api 서버가 업데이트 됐음을 알림
3. API 서버에 pod 생성 요청이 온 걸 컨트롤러 매니저가 **감지하면** 컨트롤러 매니저가 pod을 생성하고, 이 상태를 api 서버로 전달함(아직 어떤 워커 노드로 pod을 적용할지는 결정되지 않은 상태)
4. API 서버에 pod이 생성됨을 스케줄러가 **감지**, 스케줄러에서 조건을 고려해 워커 노드를 결정하고 해당 워커 노드에 새로운 pod을 넣도록 스케줄링
5. api 서버로 전달된 정보대로 지정된 워커 노드에 pod이 속한지 스케줄러가 kubelet으로 확인
6. kubelet에서 컨테이너 런타임으로 pod 생성을 요청
7. pod 생성 → pod 상태 정보 전달
8. pod 사용가능 상태

쿠버네티스는 선언적인 구조여서 각 요소가 desired status를 선언하면 current state와 비교해 추구하는 상태로 맞추기 위해 상태를 변경한다. 

다만 워커 노드는 워크플로 구조로 설계됨