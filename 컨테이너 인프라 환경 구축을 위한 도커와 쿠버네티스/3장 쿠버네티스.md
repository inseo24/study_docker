### 컨테이너 인프라 환경

리눅스 운영체제의 **커널 하나에 여러 개의 컨테이너(프로세스)**가 격리된 상태로 실행

쿠버네티스의 등장으로 효율적으로 컨테이너 인프라 환경 관리가 가능해지면서 생태계가 확장됨

### 쿠버네티스를 왜 사용할까

정확하게 얘기하면 쿠버네티스는 컨테이너 Orchestration을 위한 솔루션으로 다수의 컨테이너를 유기적으로 연결, 실행, 종료, 상태 추적 등을 도와준다. 

다른 솔루션을 제치고 쿠버네티스가 시장을 선점한 이유는 다양한 형태로 활용이 가능하기 때문이다. IT 인프라 자체를 컨테이너화할 수 있다. 

학습 곡선 자체는 어려우나

1. 매우 안정적이고
2. 확장성이 좋으며
3. 세부 설정 지원이 다양하게 있고
4. 정보량도 많고
5. 에코 파트너도 많다.

### kubeadm으로 쿠버네티스 구성하기

1. Vagrantfile 
    - vagrant up 명령어를 입력하면, vagrantfile을 읽어 정의된 가상 머신들을 생성하고 생성한 가상머신에 쿠버네티스 클러스터를 구성하기 위한 파일을 호출해 자동 구성

```bash
# -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  N = 3 # worker node의 수, args : N이 적힌 곳에서 config.sh로 넘김
  Ver = '1.18.4' # 설치할 쿠버네티스 버전 지정 

  #=============#
  # Master Node #
  #=============#

    config.vm.define "m-k8s" do |cfg|
      cfg.vm.box = "sysnet4admin/CentOS-k8s"
      cfg.vm.provider "virtualbox" do |vb|
        vb.name = "m-k8s(github_SysNet4Admin)"
        vb.cpus = 2
        vb.memory = 3072
        vb.customize ["modifyvm", :id, "--groups", "/k8s-SgMST-1.13.1(github_SysNet4Admin)"]
      end
      cfg.vm.host_name = "m-k8s"
      cfg.vm.network "private_network", ip: "192.168.1.10"
      cfg.vm.network "forwarded_port", guest: 22, host: 60010, auto_correct: true, id: "ssh"
      cfg.vm.synced_folder "../data", "/vagrant", disabled: true 
      cfg.vm.provision "shell", path: "config.sh", args: N
# 버전과 Main을 install_pkg.sh로 넘김. 
# Main문자는 install_pkg.sh에서 조건문으로 처리해 마스터 노드에만 전체 실행 코드를 내려받게함
      cfg.vm.provision "shell", path: "install_pkg.sh", args: [ Ver, "Main" ] 
      cfg.vm.provision "shell", path: "master_node.sh"
    end

  #==============#
  # Worker Nodes #
  #==============#

  (1..N).each do |i|
    config.vm.define "w#{i}-k8s" do |cfg|
      cfg.vm.box = "sysnet4admin/CentOS-k8s"
      cfg.vm.provider "virtualbox" do |vb|
        vb.name = "w#{i}-k8s(github_SysNet4Admin)"
        vb.cpus = 1
        vb.memory = 2560
        vb.customize ["modifyvm", :id, "--groups", "/k8s-SgMST-1.13.1(github_SysNet4Admin)"]
      end
      cfg.vm.host_name = "w#{i}-k8s"
      cfg.vm.network "private_network", ip: "192.168.1.10#{i}"
      cfg.vm.network "forwarded_port", guest: 22, host: "6010#{i}", auto_correct: true, id: "ssh"
      cfg.vm.synced_folder "../data", "/vagrant", disabled: true
      cfg.vm.provision "shell", path: "config.sh", args: N
      cfg.vm.provision "shell", path: "install_pkg.sh", args: Ver
      cfg.vm.provision "shell", path: "work_nodes.sh"
    end
  end

end
```

1. [config.sh](http://config.sh) 
    - kubeadm으로 쿠버네티스 설치를 위한 사전 조건 설정 스크립트 파일

```bash
#!/usr/bin/env bash

# vim config - vi를 입력하면 vim을 호출하게 설정
echo 'alias vi=vim' >> /etc/profile

# 쿠버네티스 설치 조건 상 스왑되면 안된다고 함
swapoff -a
# 시스템이 다시 시작되더라도 스왑되지 않게 설정
sed -i.bak -r 's/(.+ swap .+)/#\1/' /etc/fstab

# 쿠버네티스 레포 설정 경로를 변수로 설정
# 그 아래부턴 쿠버네티스를 내려받을 레포를 설정함
gg_pkg="packages.cloud.google.com/yum/doc" 
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://${gg_pkg}/yum-key.gpg https://${gg_pkg}/rpm-package-key.gpg
EOF

# selinux를 permissive mode로 변경(제한 풀기)
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

# IPv4 Ipv6의 패킷을 iptables가 관리하게 설정, pod의 통신을 iptable로 제어(다른 걸로 구성가능)
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
modprobe br_netfilter # br_netfilter 커널 모듈을 사용해 브리지로 네트워크 구성. 적어줘야 iptable이 활성화됨

# 쿠버네티스 안 노드 간 통신을 이름으로 할 수 있게 각 노드 호스트 이름, ip를 /etc/hosts에 설정
# 위에 N 변수가(노드 수) 여기로 전달됨
echo "192.168.1.10 m-k8s" >> /etc/hosts
for (( i=1; i<=$1; i++  )); do echo "192.168.1.10$i w$i-k8s" >> /etc/hosts; done

# config DNS  
cat <<EOF > /etc/resolv.conf
nameserver 1.1.1.1 #cloudflare DNS
nameserver 8.8.8.8 #Google DNS
EOF
```

1. install_pkg.sh
    - 클러스터 구성을 위해 가상 머신에 설치해야 할 의존성 패키지 명시

```bash
#!/usr/bin/env bash

yum install epel-release -y
yum install vim-enhanced -y
yum install git -y # 깃헙에서 내려받을 수 있게 깃 설정함

# 쿠버네티스를 관리하는 컨테이너 설치를 위해 도커 설치하고 구동
yum install docker -y && systemctl enable --now docker

# 넘겨 받은 Ver 버전의 kubectl, kubelet, kubeadm을 설치하고 kubelet을 시작함
yum install kubectl-$1 kubelet-$1 kubeadm-$1 -y
systemctl enable --now kubelet

# 전체 실행 코드를 마스터 노드만 내려받도록 vagrantfile에서 두 번쨰 변수인 Main을 넘겨받음
# 깃에서 코드를 내려받아 실습을 진행할 루트 홈 디렉터리(/root)로 옮김
# .sh를 find로 찾아 바로 실행 가능한 상태가 되도록 chmod700으로 설정
if [ $2 = 'Main' ]; then
  git clone https://github.com/sysnet4admin/_Book_k8sInfra.git
  mv /home/vagrant/_Book_k8sInfra $HOME
  find $HOME/_Book_k8sInfra/ -regex ".*\.\(sh\)" -exec chmod 700 {} \;
fi
```

1. master_node.sh
    - 1개의 가상 머신을 쿠버네티스 마스터 노드로 구성하는 스크립트
    - 여기선 CNI(쿠버네티스 네트워크 인터페이스)도 함께 구성

```bash
#!/usr/bin/env bash

# kubeadm을 통해 쿠버네티스의 워커 노드를 받을 준비
# 토큰을 123456.~ 으로 지정하고 ttl(유지 시간)을 0으로 설정 -> 기본값인 24시간 후 토큰이 계속 유지되게함
# 워커 노드가 정해진 토큰으로 들어오게함
kubeadm init --token 123456.1234567890123456 --token-ttl 0 \
--pod-network-cidr=172.16.0.0/16 --apiserver-advertise-address=192.168.1.10 

# 마스터 노드에서 현재 사용자가 쿠버네티스를 정상적으로 구동할 수 있게 설정 파일을 홈디렉터리에 복사
# 쿠버네티스를 이용할 사용자에게 권한 설정 
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

# CNI인 Calico의 설정을 적용해 쿠버네티스 네트워크를 구성 
kubectl apply -f \
https://raw.githubusercontent.com/sysnet4admin/IaC/master/manifests/172.16_net_calico.yaml
```

1. work_nodes.sh
    - 쿠버네티스 워커 노드를 구성하는 스크립트
    - 마스터 노드에 구성된 클러스터에 조인이 필요한 정보가 모두 코드화돼 있어 스크립트를 실행하기만 하면 편하게 워커 노드로서 쿠버네티스 클러스터에 조인됨

```bash
#!/usr/bin/env bash

# kubeadm을 이용해 마스터 노드에 접속
# discovery-token-unsafe로 인증을 무시하고 api 서버 주소로 접속 
kubeadm join --token 123456.1234567890123456 \
--discovery-token-unsafe-skip-ca-verification 192.168.1.10:6443
```

### pod 배포 순서에 따른 쿠버네티스 구성 요소

1. kubectl
    1. 쿠버네티스 클러스터에 명령을 내림
    2. 보통 api 서버와 주로 통신해 마스터 노드에 두기도 함(안둬도 상관 없음)(m-k8s가 마스터노드)
2. API 서버
    1. 주로 상태값을 저장하는 etcd와 통신, 그 외 API 서버 중심으로 통신하기 때문에 중심 역할
3. etcd
    1. 구성 요소들의 상태 값이 모두 저장됨. 백업 해두면 후에 그대로 쿠버네티스 클러스터 복구 가능. 
    2. key-value 저장소
4. 컨트롤러 매니저
    
    쿠버네티스 클러스터의 오브젝트 상태를 관리. 상태 체크, 복구 등
    
5. 스케줄러
    
    노드 상태, 자원 등을 고려해 어떤 worker node에 pod을 생성할 지 결정하고 할당
    
6. kubelet
    
    pod의 구성 내용(podSpec)을 컨테이너 런타임으로 전달, 모니터링함
    
7. 컨테이너 런타임(CRI, Container Runtime Interface)
    
    pod을 이루는 컨테이너를 실행하는 표준 인터페이스
    
8. pod
    
    1개 이상의 컨테이너로 일을 하기 위해 모인 단위. 언제라도 죽을 수 있음
    

1~8번은 기본 설정이지만 아래부터는 선택 설정임

1. 네트워크 플러그인
    
    클러스터 통신을 위해 플러그인을 설치. 일반적으로 CNI로 구성하는데 여기선 캘리코를 사용
    
2. CoreDNS
    
    DNS 서버
    

### 사용자가 배포된 pod에 접속할 때

1. kube-proxy
    1. kube-proxy를 이용해 pod이 위치한 노드로 통신할 수 있는 네트워크를 설정함
    2. 실제 통신은 br_netfilter, iptables로 관리함(위의 설정 파일에 있는 내용)
2. pod
    1. 이미 배포된 pod에 접속하고 필요한 내용을 전달받음
    2. 사용자는 pod이 어떤 워커 노드에 있는지 신경 안써도 됨
    

### Pod 생명주기로 쿠버네티스 구성 요소 살펴보기

1. kubectl을 통해 API 서버에 pod 생성 요청
2. (업데이트가 있을 때마다) API 서버에 전달된 내용을 etcd에 기록하고 클러스터 상태값을 최신으로 유지함, etcd도 api 서버가 업데이트 됐음을 알림
3. API 서버에 pod 생성 요청이 온 걸 컨트롤러 매니저가 **감지하면** 컨트롤러 매니저가 pod을 생성하고, 이 상태를 api 서버로 전달함(아직 어떤 워커 노드로 pod을 적용할지는 결정되지 않은 상태)
4. API 서버에 pod이 생성됨을 스케줄러가 **감지**, 스케줄러에서 조건을 고려해 워커 노드를 결정하고 해당 워커 노드에 새로운 pod을 넣도록 스케줄링
5. api 서버로 전달된 정보대로 지정된 워커 노드에 pod이 속한지 스케줄러가 kubelet으로 확인
6. kubelet에서 컨테이너 런타임으로 pod 생성을 요청
7. pod 생성 → pod 상태 정보 전달
8. pod 사용가능 상태

쿠버네티스는 선언적인 구조여서 각 요소가 desired status를 선언하면 current state와 비교해 추구하는 상태로 맞추기 위해 상태를 변경한다. 

다만 워커 노드는 워크플로 구조로 설계됨


# 8일차

### Kubectl

kubectl은 API 서버를 통해 쿠버네티스 명령을 내림

→ API 서버 접속 정보만 있으면 어디서든 kubectl 사용 가능

1. 쿠버네티스 클러스터 정보(admin.conf)를 마스터 노드에서 scp(secure copy)로 현재 디렉터리로 복사
    
    접속 기록이 없으므로 known_hosts로 저장하도록 yes로 입력하고 마스터노드 접속 암호인 패스워드에 vagrant 입력
    
    ```bash
    scp root@192.168.1.10:/ect/kubernetes/admin.conf
    ```
    

1. 쿠버네티스 클러스터 정보를 입력 받기
    
    ```bash
    kubectl get nodes --kubeconfig admin.conf
    ```
    

### Kubelet

kubelet은 pod 생성, 상태 관리, 복구 등을 담당

→ 즉, kubelet이 없으면 pod 생성, 삭제 등에 문제가 생김

1. 기능 검증을 위해 실제 pod 배포
    
    nginx 웹 pod을 배포함. 아래의 -f는 force가 아니라 filename을 의미하는 옵션
    
    파일을 읽어 1개의 pod을 임의의 워커노드에 배포함
    
    ```bash
    kubectl create -f ~/_Book_k8sInfra/~~~~~/nginx-pod.yaml
    ```
    

1. 정상 배포 상태인지 확인(Running)
    
    ```bash
    kubectl get pod
    ```
    
2. pod이 배포된 워커 노드를 확인
    
    -o: output, 출력을 특정 형식으로 해주는 옵션
    
    wide : 더 많은 정보를 제공
    
    ```bash
    get pods -o wid
    ```
    
3. 배포 노드에 접속해 kubelet 서비스를 멈춤
    
    ```bash
    systemctl stop kubelet
    ```
    
4. 마스터 노드로 돌아가서 get pod으로 상태를 확인하고 nginx pod을 삭제함
    
    ```bash
    kubectl get pod
    kubectl delete pod nginx-pod
    ```
    
5. 삭제 명령이 너무 오래 걸리면 Ctrl+c로 삭제 명령을 중지할 수 있음
    1. 실행 결과를 보면 pod을 terminating(삭제)하고 있음을 알 수 있으나, kubelet이 작동하지 않는 상태라 pod은 삭제 되지 않음
    2. 다시 배포 노드에서 kubelet을 복구함
        
        ```bash
        systemctl start kubelet
        ```
        
    3. 이후 마스터노드에서 get pod으로 상태를 확인하면 nginx pod이 삭제됨을 확인할 수 있음
    

### kube-proxy

kube-proxy는 pod의 통신을 담당함

여기선 [config.sh](http://config.sh) 파일에서 br_netfilter 커널 모듈을 적재하고 iptables를 거쳐 통신하도록 설정됨

→ 설정이 정상 작동하지 않는다면, pod의 웹 서버 등에선 정상적으로 데이터를 받아오지 못함

1. 기능 검증을 위해 마스터 노드에서 pod 배포
    
    ```bash
    kubectl create -f ~/~~~~/nginx-pod.yaml
    ```
    
2. pod의 IP와 워커 노드 확인
    
    ```bash
    kubectl get pod -o wide
    ```
    
3. curl(client URL)로 pod의 IP로 nginx 웹 서버 메인 페이지 내용을 확인
    
    ```bash
    curl 172.16.103.130
    ```
    
4. 배포 노드에 접속해 워커 노드의 br_netfilter 모듈을 제거 후 네트워크를 다시 시작해 변경 사항을 적용
    
    ```bash
    modprobe -r br_netfilter
    systemctl restart network
    ```
    
5. 마스터노드에서 curl로 다시 nginx 웹 서버 메인 페이지 내용을 확인하려고 하면 받아오지 못함
    
    ```bash
    curl 172.16.103.130
    ```
    
6. pod 상태를 확인하면 kubelet을 통해 확인할 때 pod, node, ip 그대로고 running이 떠서 문제가 없는 것으로 보이나 실제로는 kube-proxy가 이용중인 br_netfilter에 문제가 있어 pod과 nginx의 웹 서버와의 통신만이 정상적으로 이뤄지지 않은 상태임(curl로 nginx 서버 접속했으나 연결되지 않음, connection time out)
    
    ```bash
    kubectl get pod -o wide
    ```
    
7. 정상적으로 다시 받아올 수 있게 워커 노드에서 br_netfilter 모듈을 다시 커널에 적재하고 재시작함
    
    ```bash
    modprobe br_netfilter
    reboot
    ```
    
8. 마스터노드에서 pod 상태 확인하면 1회 다시 시작했다는 의미로 RESTARTS가 1 증가하고, IP가 변경됨을 확인할 수 있음, curl로 다시 데이터 받아와도 정상적으로 불러와짐.


### 레플리카셋으로 pod 수 관리하기

1. 배포된 pod 중 하나를 scale 명령으로 3개로 증가시킴
    
    ```bash
    kubectl scale pod nginx-pod --replicas=3
    ```
    
    실행하면 리소스를 찾을 수 없다는 에러 메시지가 발생 → nginx-pod는 pod으로 생성되어서 deployment 오브젝트에 속하지 않음
    
2. 디플로이먼트로 생성된 pod을 scale 명령으로 3개로 증가시킴
    
    ```bash
    kubectl scale deployment dpy-nginx --replicas=3
    ```
    

### 스펙을 지정해 오브젝트 생성하기

디플로이먼트를 생성하면서 한꺼번에 여러 개의 pod를 만들 수 있다.

이런 설정을 적용하려면 필요한 내용을 파일로 작성해야 하는데 이 때 작성하는 파일을 오브젝트 spec 이라고 한다. 일반적으로 YAML 파일로 작성한다.

echo-hname.yaml

```yaml
apiVersion: apps/v1 #API version
kind: Deployment # 오브젝트 종류
metadata:
	name: echo-hname # deployment의 이름
	labels:  # deployment의 레이블
		app: nginx
spec:
	replicas: 3 # 생성할 pod 수
	selector:  # 셀렉터의 레이블 지정
		matchLabels: 
			app: nginx
		template:   # 템플릿의 레이블 지정
			metadata:
				labels:
					app: nginx
			spec:   # 템플릿에 사용할 컨테이너 이미지 지정
				containers:
				- name: echo-hname
					image: sysnet4admin/echo-hname # 사용되는 이미지
```

nginx-pod.yaml

```yaml
apiVersion: v1
kind: Pod
metadata: 
	name: nginx-pod # pod의 이름
spec: #pod에서 호출할 컨테이너 이미지 지정
	containers:
	- name: container-name
		image: nginx
```

1. 파일을 이용해 디플로이먼트 생성하기
    
    ```bash
    kubectl create -f ~/_~~~/echo-hname.yaml
    ```
    
2. yaml 파일을 수정해 pod 수를 6개로 늘려보기 (replicas 값을 6으로 변경)
    
    sed(streamlined editor) → 이거 말고 vim editor로 바로 수정해도 됨
    
    -i : —in-place의 약어로 변경한 내용을 현재 파일에 바로 적용함을 뜻함
    
    s/ : 주어진 패턴을 원하는 대로 변경함
    
    ```bash
    sed -i 's/replicas: 3/replicas: 6/' ~/~~~~/echo-hname.yaml
    ```
    
3. 변경 확인
    
    ```bash
    cat ~/~~~/echo-hname.yaml | grep replicas
    replicas: 6
    ```
    
4. 변경 내용 적용
    
    ```bash
    kubectl create -f ~/_~~~/echo-hname.yaml
    ```
    
    echo-hname 이 이미 존재한다는 에러 메시지가 나오면서  pod 수가 늘어나지 않음
    
    → apply로 오브젝트 생성하고 관리하는게 좋음
    

### 오브젝트 생성 명령어 비교

| 구분 | Run | Create | Apply |
| --- | --- | --- | --- |
| 명령 실행 | 제한적임 | 가능함 | 안 됨 |
| 파일 실행 | 안 됨 | 가능함 | 가능함 |
| 변경 가능 | 안 됨 | 안 됨 | 가능함 |
| 실행 편의성 | 매우 좋음 | 매우 좋음 | 좋음 |
| 기능 유지 | 제한적임 | 지원됨 | 다양하게 지원됨 |

위에서 본대로 create는 파일의 변경 사항을 바로 적용할 수 없다. 이럴 경우를 위해 apply로 오브젝트를 관리할 수 있다.

수정한 yaml파일에 apply 명령으로 적용

```bash
kubectl apply -f ~/~~~/echo-hname.yaml
```

오브젝트를 처음부터 apply로 생성한 것이 아니라 경고가 뜨긴 하나 작동은 함.

변경 사항이 발생할 경우를 대비해서 처음부터 apply로 생성하는 것이 좋음

ad-hoc(일회성 사용)으로 오브젝트를 생성할 때는 create, 변경이 생길 가능성이 있는 복잡한 오브젝트는 파일로 작성 후 apply로 적용하는 것이 좋다.

### pod의 컨테이너 자동 복구(self-healing)

1. pod 접속을 위해 IP를 확인(kubectl get pods -o wide)
2. exec를 통해 pod 컨테이너의 셸에 접속
    
    i : stdin(standard input)
    
    t : tty(teletypewriter, 명령줄 인터페이스)
    
    it : 표준 입력을 명령줄 인터페이스로 작성함
    
    nginx-pod의 /bin/bash를 실행해 nginx-pod의 컨테이너에서 배시 셸에 접속
    
    ```bash
    kubectl exec -it nginx-pod -- /bin/bash
    ```
    
3. 컨테이너에서 구동하는 nginx의 PID(Process ID)를 확인
    
    nginx의 PID는 언제나 1임
    
    ```bash
    cat /run/nginx.pid
    1
    ```
    

1. 슈퍼푸티에서 m-k8s의 터미널을 1개 더 띄워 nginx-pod의 IP에서 돌아가는 웹 페이지를 1초에 한 번씩 요청하는 스크립트를 실행. curl에서 요청한 값만 받도록 —silient 옵션을 추가
    
    ```bash
    i=1; while true; do sleep 1; echo  $((i++) `curl --silent
    ```
    
2. 배시 셸에서 PID 1번을 kill
    
    ```bash
    kill 1
    ```
    
3. nginx 웹 페이지를 받아오는 스크립트가 잘 작동하는지 확인 → 자동으로 복구되는 것 확인 가능
    
    nginx.pid가 생성된 시간으로 새로 생성된 프로세스인지 확인함
    
    ```bash
    kubectl exec -it nginx-pod -- /bin/bash
    ls -l run/nginx.pid
    ```


### pod의 동작 보증 기능 -  deployment 자동 복구

pod에 문제 발생 시 자동 복구해 pod이 항상 동작하도록 보장함

1. 문제 상황 만들기 - pod 삭제
    
    ```bash
    # pod 확인
    kubectl get pods
    
    # delete
    kubectl delete pods nginx-pod
    ```
    

1. pod 동작 보증을 위한 조건 확인 - 다른 pod도 삭제해서 비교해보기 위해 삭제
    
    ```bash
    kubectl delete pods echo-hname-5dakdjjk39-2jvaj
    ```
    

1. 삭제가 잘 됐나 확인하면 기존 pod이 다시 새로 생성됨을 확인할 수 있음(AGE)
    
    ```bash
    kubectl get pods
    ```
    

nginx-pod은 디플로이먼트에 속한 pod이 아니기 때문에 현재 관리되고 있지 않아서 삭제해도 다시 생성되지 않음

반면 echo-hname은 디플로이먼트에 속한 pod이고, echo-hname에 속한 pod에서 replicas를 6개로 선언했기 때문에 그 선언한 수대로 pod 수가 유지된다. → 따라서 임의의 pod을 삭제해도 replicas가 이를 확인하고 새로운 pod을 수에 맞게 생성함

디플로이먼트로 생성하는 것이 pod 동작을 보증하기 위한 조건 중 하나임.

그럼에도 디플로이먼트의 pod을 삭제하고 싶다면 상위 디플로이먼트를 삭제해야 한다.

```bash
# deployment 삭제
kubectl delete deployment echo-hname

# pod 확인
kubectl get pods
```

### 노드 자원 보호 - cordon 기능

노드 : 쿠버네티스 스케줄러에서 pod을 할당 받고 처리

쿠버네티스에선 문제가 생길 가능성이 있는 노드임을 알리기 위해 cordon 기능을 사용해 관리함

1. pod 생성
    
    ```bash
    kubectl apply -f ~/~~~/echo-hname.yaml
    ```
    

1. scale 명령으로 배포한 pod을 늘림
    
    ```bash
    kubectl scale deployment echo-name --replicas=9
    ```
    
2. 배포된 pod 확인
    
    custom-columns : 임의로 구성할 수 있음
    
    ```bash
    kubectl get pods -o=custom-columns=NAME:.metadata.name, IP:.status.podIP, 
    STATUS:.status.phase, NODE:.spec.nodeName
    ```
    
3. scale로 pod 수를 3개로 줄임
    
    ```bash
    kubectl scale deployment echo-hname --replicas=3
    ```
    
4. 각 노드에 pod 이 1개만 남았는지 확인 - 3번
5. w3-k8s 노드에서 문제가 자주 발생해 현재 상태를 보존한다고 가정하고 → w3-k8s 노드에서 cordon 명령을 실행
    
    ```bash
    # 추가로 pod 배포 시 w3-k8s 노드로는 할당되지 않음
    kubectl cordon w3-k8s
    ```
    
6. kubectl get nodes 명령을 실행해 cordon 명령이 제대로 적용됐는지 확인
    
    ```bash
    kubectl get nodes
    # SchedulingDisable 표시가 뜨면 변경된 것임
    ```
    

1. 이 상태에서 pod 수를 9개로 늘리고 확인
    
    ```bash
    kubectl scale deployment echo-hname --replicas=9
    kubectl get pods ~~ # 1개만 w3-k8s 노드라면 정상
    ```
    

1. pod 할당되지 않게 설정했던 것 해제
    
    ```bash
    kubectl uncordon w3-k8s
    ```
    

### 노드 유지보수하기 - drain 기능

drain : 지정된 노드의 pod를 전부 다른 곳으로 이동시켜 해당 노드를 유지보수 할 수 있게 제공

1. 유지보수할 노드(w3-k8s)를 pod이 없는 상태로 만듬 
    
    → 데몬셋을 지울 수 없어 명령을 수행할 수 없다고 출력
    
    ```bash
    kubectl drain w3-k8s
    error: unable to drain node "w3-k8s", aborting command...
    
    There are pending nodes to be drained:
     w3-k8s
    error: cannot delete DaemonSet-managed Pods (use --ignore-deaemonsets to ignore)
    : kube-system/calico-node-j9pic, kube-system/kube-proxy-5ltsx
    ```
    
    → drain은 실제 pod을 옮기는 것이 아니라 노드에서 pod을 삭제하고 다른 곳에 다시 생성함
    
    → DaemonSet은 각 노드에 1개만 존재하는 pod이라 drain으로 삭제할 수 없음
    
2. ignore 와 함께 사용
    
    ```bash
    kubectl drain w3-k8s --ignore-daemonsets
    ```
    
3. 노드에 pod이 없는지 확인 + 옮긴 노드에 pod이 새로 생성돼 pod 이름, IP 부여된 것 확인
    
    ```bash
    kubectl get pods -o=custom-columns=NAME:.metadata.name, IP:.status.podIP, 
    STATUS:.status.phase, NODE:.spec.nodeName
    ```
    
4. 노드 상태를 확인해보면 해당 노드가 cordon 때와 마찬가지로 SchedulingDiabled 상태임을 확인가능
    
    ```bash
    kubectl get nodes
    ```
    
5. 유지보수가 끝났다고 생각하면 uncordon 명령으로 스케줄을 받을 수 있는 상태로 복귀
    
    ```bash
    kubectl uncordon w3-k8s
    ```

### pod 업데이트하고 복구하기

업데이트 상황 

1. 새로운 기능 추가
2. 버그 발생해 버전 업데이트
3. 업데이트 중 문제 발생해 기존 버전으로 복구

pod 업데이트하기

1. 배포할 때 배포한 정보 히스토리를 기록하는 옵션 추가
    
    —record : 배포한 정보의 히스토리 기록
    
    ```bash
    kubectl apply -f ~/경로~/rollout-nginx.yaml --record
    ```
    
    rollout-nginx.yaml
    
    ```bash
    spec:
    	spec:
    		containers:
    		- name: nginx
    			image: nginx:1.15.12 # 템플릿에서 사용할 컨테이너 이미지 및 버전 지정
    ```
    
2. record 옵션으로 기록된 히스토리는 rollout history 명령을 실행해 확인할 수 있음
    
    ```bash
    kubectl rollout history deployment rollout-nginx
    ```
    

3. 배포한 pod 정보 확인

```bash
kubectl get pods \
-o-custom-columns:.metadata.name, IP:.status.podIP, STATUS:.status.phase, NODE:.sepc.nodeName
```

4. 배포한 pod 정보 확인
    
    ```bash
    curl -I --silent 172.16.103.143 | grep Server # -I : header 정보만 출력하는 옵션
    Server: nginx/1.15.12
    ```
    
5. nginx 컨테이너 버전을 1.16.0으로 업데이트하고 정보 확인
    
    ```bash
    kubectl set image deployment rollout-nginx nginx=nginx:1.16.0 --record
    
    kubectl get pods \
    -o-custom-columns:.metadata.name, IP:.status.podIP, STATUS:.status.phase, NODE:.sepc.nodeName
    ```
    
    - 정보를 확인하면 pod 이름과 IP가 변경됨을 확인할 수 있음 → 업데이트하면서 replicas에 속한 pods를 순차적으로 한 개씩 지웠다가 생성한다.
    - 업데이트 기본값은 25%, 최소값은 1개

6. 모두 업데이트된 후 디플로이먼트 상태 확인
    
    ```bash
    kubectl rollout status deployment rollout-nginx
    ```
    
7. rollout-nginx에 적용된 명령들 확인
    
    ```bash
    kubectl rollout history deployment rollout-nginx
    
    # server version 확인
    curl -I --silent 172.16.132.10 | grep Server
    ```
    

### 업데이트 실패 시 pod 복구하기

예를 들어, 없는 버전의 image를 생성하려고 시도한다면 디플로이먼트가 배포하는 단계에서 waiting으로 더 진행되지 않음 → 결국 에러 메시지 출력

관련 문제는 describe 명령으로 자세히 확인할 수 있음

```bash
kubectl describe deployment rollout-nginx
```

→ 이런 실수를 할 수 있기 때문에 업데이트 할 때 rollout을 사용하고 —record로 기록함

1. 복구를 위해서 rollout history를 확인
    
    ```bash
    kubectl rollout history deployment rollout-nginx
    ```
    

2. 명령 실행을 취소해 마지막 단계(revision 3)에서 전 단계(revision 2)로 상태를 되돌림
    
    ```bash
    kubectl rollout undo deployment rollout-nginx
    ```
    

3. pod 상태 확인
    
    ```bash
    kubectl get pods \
    -o-custom-columns:.metadata.name, IP:.status.podIP, STATUS:.status.phase, NODE:.sepc.nodeName
    ```
    
    - revision 4가 생성되고 revision 2가 삭제됨을 확인할 수 있음
    
4. 변경이 정상적으로 적용됐는지 확인
    
    ```bash
    kubectl rollout status deployment rollout-nginx
    ```
    

### 특정 시점으로 pod 복구

—to-revision 옵션 사용

```bash
kubectl rollout undo deployment rollout-nginx --to-revision=1
```