### 컨테이너 인프라 환경

리눅스 운영체제의 **커널 하나에 여러 개의 컨테이너(프로세스)**가 격리된 상태로 실행

쿠버네티스의 등장으로 효율적으로 컨테이너 인프라 환경 관리가 가능해지면서 생태계가 확장됨

### 쿠버네티스를 왜 사용할까

정확하게 얘기하면 쿠버네티스는 컨테이너 Orchestration을 위한 솔루션으로 다수의 컨테이너를 유기적으로 연결, 실행, 종료, 상태 추적 등을 도와준다. 

다른 솔루션을 제치고 쿠버네티스가 시장을 선점한 이유는 다양한 형태로 활용이 가능하기 때문이다. IT 인프라 자체를 컨테이너화할 수 있다. 

학습 곡선 자체는 어려우나

1. 매우 안정적이고
2. 확장성이 좋으며
3. 세부 설정 지원이 다양하게 있고
4. 정보량도 많고
5. 에코 파트너도 많다.

### kubeadm으로 쿠버네티스 구성하기

1. Vagrantfile 
    - vagrant up 명령어를 입력하면, vagrantfile을 읽어 정의된 가상 머신들을 생성하고 생성한 가상머신에 쿠버네티스 클러스터를 구성하기 위한 파일을 호출해 자동 구성

```bash
# -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  N = 3 # worker node의 수, args : N이 적힌 곳에서 config.sh로 넘김
  Ver = '1.18.4' # 설치할 쿠버네티스 버전 지정 

  #=============#
  # Master Node #
  #=============#

    config.vm.define "m-k8s" do |cfg|
      cfg.vm.box = "sysnet4admin/CentOS-k8s"
      cfg.vm.provider "virtualbox" do |vb|
        vb.name = "m-k8s(github_SysNet4Admin)"
        vb.cpus = 2
        vb.memory = 3072
        vb.customize ["modifyvm", :id, "--groups", "/k8s-SgMST-1.13.1(github_SysNet4Admin)"]
      end
      cfg.vm.host_name = "m-k8s"
      cfg.vm.network "private_network", ip: "192.168.1.10"
      cfg.vm.network "forwarded_port", guest: 22, host: 60010, auto_correct: true, id: "ssh"
      cfg.vm.synced_folder "../data", "/vagrant", disabled: true 
      cfg.vm.provision "shell", path: "config.sh", args: N
# 버전과 Main을 install_pkg.sh로 넘김. 
# Main문자는 install_pkg.sh에서 조건문으로 처리해 마스터 노드에만 전체 실행 코드를 내려받게함
      cfg.vm.provision "shell", path: "install_pkg.sh", args: [ Ver, "Main" ] 
      cfg.vm.provision "shell", path: "master_node.sh"
    end

  #==============#
  # Worker Nodes #
  #==============#

  (1..N).each do |i|
    config.vm.define "w#{i}-k8s" do |cfg|
      cfg.vm.box = "sysnet4admin/CentOS-k8s"
      cfg.vm.provider "virtualbox" do |vb|
        vb.name = "w#{i}-k8s(github_SysNet4Admin)"
        vb.cpus = 1
        vb.memory = 2560
        vb.customize ["modifyvm", :id, "--groups", "/k8s-SgMST-1.13.1(github_SysNet4Admin)"]
      end
      cfg.vm.host_name = "w#{i}-k8s"
      cfg.vm.network "private_network", ip: "192.168.1.10#{i}"
      cfg.vm.network "forwarded_port", guest: 22, host: "6010#{i}", auto_correct: true, id: "ssh"
      cfg.vm.synced_folder "../data", "/vagrant", disabled: true
      cfg.vm.provision "shell", path: "config.sh", args: N
      cfg.vm.provision "shell", path: "install_pkg.sh", args: Ver
      cfg.vm.provision "shell", path: "work_nodes.sh"
    end
  end

end
```

1. [config.sh](http://config.sh) 
    - kubeadm으로 쿠버네티스 설치를 위한 사전 조건 설정 스크립트 파일

```bash
#!/usr/bin/env bash

# vim config - vi를 입력하면 vim을 호출하게 설정
echo 'alias vi=vim' >> /etc/profile

# 쿠버네티스 설치 조건 상 스왑되면 안된다고 함
swapoff -a
# 시스템이 다시 시작되더라도 스왑되지 않게 설정
sed -i.bak -r 's/(.+ swap .+)/#\1/' /etc/fstab

# 쿠버네티스 레포 설정 경로를 변수로 설정
# 그 아래부턴 쿠버네티스를 내려받을 레포를 설정함
gg_pkg="packages.cloud.google.com/yum/doc" 
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://${gg_pkg}/yum-key.gpg https://${gg_pkg}/rpm-package-key.gpg
EOF

# selinux를 permissive mode로 변경(제한 풀기)
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

# IPv4 Ipv6의 패킷을 iptables가 관리하게 설정, pod의 통신을 iptable로 제어(다른 걸로 구성가능)
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
modprobe br_netfilter # br_netfilter 커널 모듈을 사용해 브리지로 네트워크 구성. 적어줘야 iptable이 활성화됨

# 쿠버네티스 안 노드 간 통신을 이름으로 할 수 있게 각 노드 호스트 이름, ip를 /etc/hosts에 설정
# 위에 N 변수가(노드 수) 여기로 전달됨
echo "192.168.1.10 m-k8s" >> /etc/hosts
for (( i=1; i<=$1; i++  )); do echo "192.168.1.10$i w$i-k8s" >> /etc/hosts; done

# config DNS  
cat <<EOF > /etc/resolv.conf
nameserver 1.1.1.1 #cloudflare DNS
nameserver 8.8.8.8 #Google DNS
EOF
```

1. install_pkg.sh
    - 클러스터 구성을 위해 가상 머신에 설치해야 할 의존성 패키지 명시

```bash
#!/usr/bin/env bash

yum install epel-release -y
yum install vim-enhanced -y
yum install git -y # 깃헙에서 내려받을 수 있게 깃 설정함

# 쿠버네티스를 관리하는 컨테이너 설치를 위해 도커 설치하고 구동
yum install docker -y && systemctl enable --now docker

# 넘겨 받은 Ver 버전의 kubectl, kubelet, kubeadm을 설치하고 kubelet을 시작함
yum install kubectl-$1 kubelet-$1 kubeadm-$1 -y
systemctl enable --now kubelet

# 전체 실행 코드를 마스터 노드만 내려받도록 vagrantfile에서 두 번쨰 변수인 Main을 넘겨받음
# 깃에서 코드를 내려받아 실습을 진행할 루트 홈 디렉터리(/root)로 옮김
# .sh를 find로 찾아 바로 실행 가능한 상태가 되도록 chmod700으로 설정
if [ $2 = 'Main' ]; then
  git clone https://github.com/sysnet4admin/_Book_k8sInfra.git
  mv /home/vagrant/_Book_k8sInfra $HOME
  find $HOME/_Book_k8sInfra/ -regex ".*\.\(sh\)" -exec chmod 700 {} \;
fi
```

1. master_node.sh
    - 1개의 가상 머신을 쿠버네티스 마스터 노드로 구성하는 스크립트
    - 여기선 CNI(쿠버네티스 네트워크 인터페이스)도 함께 구성

```bash
#!/usr/bin/env bash

# kubeadm을 통해 쿠버네티스의 워커 노드를 받을 준비
# 토큰을 123456.~ 으로 지정하고 ttl(유지 시간)을 0으로 설정 -> 기본값인 24시간 후 토큰이 계속 유지되게함
# 워커 노드가 정해진 토큰으로 들어오게함
kubeadm init --token 123456.1234567890123456 --token-ttl 0 \
--pod-network-cidr=172.16.0.0/16 --apiserver-advertise-address=192.168.1.10 

# 마스터 노드에서 현재 사용자가 쿠버네티스를 정상적으로 구동할 수 있게 설정 파일을 홈디렉터리에 복사
# 쿠버네티스를 이용할 사용자에게 권한 설정 
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

# CNI인 Calico의 설정을 적용해 쿠버네티스 네트워크를 구성 
kubectl apply -f \
https://raw.githubusercontent.com/sysnet4admin/IaC/master/manifests/172.16_net_calico.yaml
```

1. work_nodes.sh
    - 쿠버네티스 워커 노드를 구성하는 스크립트
    - 마스터 노드에 구성된 클러스터에 조인이 필요한 정보가 모두 코드화돼 있어 스크립트를 실행하기만 하면 편하게 워커 노드로서 쿠버네티스 클러스터에 조인됨

```bash
#!/usr/bin/env bash

# kubeadm을 이용해 마스터 노드에 접속
# discovery-token-unsafe로 인증을 무시하고 api 서버 주소로 접속 
kubeadm join --token 123456.1234567890123456 \
--discovery-token-unsafe-skip-ca-verification 192.168.1.10:6443
```

### pod 배포 순서에 따른 쿠버네티스 구성 요소

1. kubectl
    1. 쿠버네티스 클러스터에 명령을 내림
    2. 보통 api 서버와 주로 통신해 마스터 노드에 두기도 함(안둬도 상관 없음)(m-k8s가 마스터노드)
2. API 서버
    1. 주로 상태값을 저장하는 etcd와 통신, 그 외 API 서버 중심으로 통신하기 때문에 중심 역할
3. etcd
    1. 구성 요소들의 상태 값이 모두 저장됨. 백업 해두면 후에 그대로 쿠버네티스 클러스터 복구 가능. 
    2. key-value 저장소
4. 컨트롤러 매니저
    
    쿠버네티스 클러스터의 오브젝트 상태를 관리. 상태 체크, 복구 등
    
5. 스케줄러
    
    노드 상태, 자원 등을 고려해 어떤 worker node에 pod을 생성할 지 결정하고 할당
    
6. kubelet
    
    pod의 구성 내용(podSpec)을 컨테이너 런타임으로 전달, 모니터링함
    
7. 컨테이너 런타임(CRI, Container Runtime Interface)
    
    pod을 이루는 컨테이너를 실행하는 표준 인터페이스
    
8. pod
    
    1개 이상의 컨테이너로 일을 하기 위해 모인 단위. 언제라도 죽을 수 있음
    

1~8번은 기본 설정이지만 아래부터는 선택 설정임

1. 네트워크 플러그인
    
    클러스터 통신을 위해 플러그인을 설치. 일반적으로 CNI로 구성하는데 여기선 캘리코를 사용
    
2. CoreDNS
    
    DNS 서버
    

### 사용자가 배포된 pod에 접속할 때

1. kube-proxy
    1. kube-proxy를 이용해 pod이 위치한 노드로 통신할 수 있는 네트워크를 설정함
    2. 실제 통신은 br_netfilter, iptables로 관리함(위의 설정 파일에 있는 내용)
2. pod
    1. 이미 배포된 pod에 접속하고 필요한 내용을 전달받음
    2. 사용자는 pod이 어떤 워커 노드에 있는지 신경 안써도 됨
    

### Pod 생명주기로 쿠버네티스 구성 요소 살펴보기

1. kubectl을 통해 API 서버에 pod 생성 요청
2. (업데이트가 있을 때마다) API 서버에 전달된 내용을 etcd에 기록하고 클러스터 상태값을 최신으로 유지함, etcd도 api 서버가 업데이트 됐음을 알림
3. API 서버에 pod 생성 요청이 온 걸 컨트롤러 매니저가 **감지하면** 컨트롤러 매니저가 pod을 생성하고, 이 상태를 api 서버로 전달함(아직 어떤 워커 노드로 pod을 적용할지는 결정되지 않은 상태)
4. API 서버에 pod이 생성됨을 스케줄러가 **감지**, 스케줄러에서 조건을 고려해 워커 노드를 결정하고 해당 워커 노드에 새로운 pod을 넣도록 스케줄링
5. api 서버로 전달된 정보대로 지정된 워커 노드에 pod이 속한지 스케줄러가 kubelet으로 확인
6. kubelet에서 컨테이너 런타임으로 pod 생성을 요청
7. pod 생성 → pod 상태 정보 전달
8. pod 사용가능 상태

쿠버네티스는 선언적인 구조여서 각 요소가 desired status를 선언하면 current state와 비교해 추구하는 상태로 맞추기 위해 상태를 변경한다. 

다만 워커 노드는 워크플로 구조로 설계됨


# 8일차

### Kubectl

kubectl은 API 서버를 통해 쿠버네티스 명령을 내림

→ API 서버 접속 정보만 있으면 어디서든 kubectl 사용 가능

1. 쿠버네티스 클러스터 정보(admin.conf)를 마스터 노드에서 scp(secure copy)로 현재 디렉터리로 복사
    
    접속 기록이 없으므로 known_hosts로 저장하도록 yes로 입력하고 마스터노드 접속 암호인 패스워드에 vagrant 입력
    
    ```bash
    scp root@192.168.1.10:/ect/kubernetes/admin.conf
    ```
    

1. 쿠버네티스 클러스터 정보를 입력 받기
    
    ```bash
    kubectl get nodes --kubeconfig admin.conf
    ```
    

### Kubelet

kubelet은 pod 생성, 상태 관리, 복구 등을 담당

→ 즉, kubelet이 없으면 pod 생성, 삭제 등에 문제가 생김

1. 기능 검증을 위해 실제 pod 배포
    
    nginx 웹 pod을 배포함. 아래의 -f는 force가 아니라 filename을 의미하는 옵션
    
    파일을 읽어 1개의 pod을 임의의 워커노드에 배포함
    
    ```bash
    kubectl create -f ~/_Book_k8sInfra/~~~~~/nginx-pod.yaml
    ```
    

1. 정상 배포 상태인지 확인(Running)
    
    ```bash
    kubectl get pod
    ```
    
2. pod이 배포된 워커 노드를 확인
    
    -o: output, 출력을 특정 형식으로 해주는 옵션
    
    wide : 더 많은 정보를 제공
    
    ```bash
    get pods -o wid
    ```
    
3. 배포 노드에 접속해 kubelet 서비스를 멈춤
    
    ```bash
    systemctl stop kubelet
    ```
    
4. 마스터 노드로 돌아가서 get pod으로 상태를 확인하고 nginx pod을 삭제함
    
    ```bash
    kubectl get pod
    kubectl delete pod nginx-pod
    ```
    
5. 삭제 명령이 너무 오래 걸리면 Ctrl+c로 삭제 명령을 중지할 수 있음
    1. 실행 결과를 보면 pod을 terminating(삭제)하고 있음을 알 수 있으나, kubelet이 작동하지 않는 상태라 pod은 삭제 되지 않음
    2. 다시 배포 노드에서 kubelet을 복구함
        
        ```bash
        systemctl start kubelet
        ```
        
    3. 이후 마스터노드에서 get pod으로 상태를 확인하면 nginx pod이 삭제됨을 확인할 수 있음
    

### kube-proxy

kube-proxy는 pod의 통신을 담당함

여기선 [config.sh](http://config.sh) 파일에서 br_netfilter 커널 모듈을 적재하고 iptables를 거쳐 통신하도록 설정됨

→ 설정이 정상 작동하지 않는다면, pod의 웹 서버 등에선 정상적으로 데이터를 받아오지 못함

1. 기능 검증을 위해 마스터 노드에서 pod 배포
    
    ```bash
    kubectl create -f ~/~~~~/nginx-pod.yaml
    ```
    
2. pod의 IP와 워커 노드 확인
    
    ```bash
    kubectl get pod -o wide
    ```
    
3. curl(client URL)로 pod의 IP로 nginx 웹 서버 메인 페이지 내용을 확인
    
    ```bash
    curl 172.16.103.130
    ```
    
4. 배포 노드에 접속해 워커 노드의 br_netfilter 모듈을 제거 후 네트워크를 다시 시작해 변경 사항을 적용
    
    ```bash
    modprobe -r br_netfilter
    systemctl restart network
    ```
    
5. 마스터노드에서 curl로 다시 nginx 웹 서버 메인 페이지 내용을 확인하려고 하면 받아오지 못함
    
    ```bash
    curl 172.16.103.130
    ```
    
6. pod 상태를 확인하면 kubelet을 통해 확인할 때 pod, node, ip 그대로고 running이 떠서 문제가 없는 것으로 보이나 실제로는 kube-proxy가 이용중인 br_netfilter에 문제가 있어 pod과 nginx의 웹 서버와의 통신만이 정상적으로 이뤄지지 않은 상태임(curl로 nginx 서버 접속했으나 연결되지 않음, connection time out)
    
    ```bash
    kubectl get pod -o wide
    ```
    
7. 정상적으로 다시 받아올 수 있게 워커 노드에서 br_netfilter 모듈을 다시 커널에 적재하고 재시작함
    
    ```bash
    modprobe br_netfilter
    reboot
    ```
    
8. 마스터노드에서 pod 상태 확인하면 1회 다시 시작했다는 의미로 RESTARTS가 1 증가하고, IP가 변경됨을 확인할 수 있음, curl로 다시 데이터 받아와도 정상적으로 불러와짐.


### 레플리카셋으로 pod 수 관리하기

1. 배포된 pod 중 하나를 scale 명령으로 3개로 증가시킴
    
    ```bash
    kubectl scale pod nginx-pod --replicas=3
    ```
    
    실행하면 리소스를 찾을 수 없다는 에러 메시지가 발생 → nginx-pod는 pod으로 생성되어서 deployment 오브젝트에 속하지 않음
    
2. 디플로이먼트로 생성된 pod을 scale 명령으로 3개로 증가시킴
    
    ```bash
    kubectl scale deployment dpy-nginx --replicas=3
    ```
    

### 스펙을 지정해 오브젝트 생성하기

디플로이먼트를 생성하면서 한꺼번에 여러 개의 pod를 만들 수 있다.

이런 설정을 적용하려면 필요한 내용을 파일로 작성해야 하는데 이 때 작성하는 파일을 오브젝트 spec 이라고 한다. 일반적으로 YAML 파일로 작성한다.

echo-hname.yaml

```yaml
apiVersion: apps/v1 #API version
kind: Deployment # 오브젝트 종류
metadata:
	name: echo-hname # deployment의 이름
	labels:  # deployment의 레이블
		app: nginx
spec:
	replicas: 3 # 생성할 pod 수
	selector:  # 셀렉터의 레이블 지정
		matchLabels: 
			app: nginx
		template:   # 템플릿의 레이블 지정
			metadata:
				labels:
					app: nginx
			spec:   # 템플릿에 사용할 컨테이너 이미지 지정
				containers:
				- name: echo-hname
					image: sysnet4admin/echo-hname # 사용되는 이미지
```

nginx-pod.yaml

```yaml
apiVersion: v1
kind: Pod
metadata: 
	name: nginx-pod # pod의 이름
spec: #pod에서 호출할 컨테이너 이미지 지정
	containers:
	- name: container-name
		image: nginx
```

1. 파일을 이용해 디플로이먼트 생성하기
    
    ```bash
    kubectl create -f ~/_~~~/echo-hname.yaml
    ```
    
2. yaml 파일을 수정해 pod 수를 6개로 늘려보기 (replicas 값을 6으로 변경)
    
    sed(streamlined editor) → 이거 말고 vim editor로 바로 수정해도 됨
    
    -i : —in-place의 약어로 변경한 내용을 현재 파일에 바로 적용함을 뜻함
    
    s/ : 주어진 패턴을 원하는 대로 변경함
    
    ```bash
    sed -i 's/replicas: 3/replicas: 6/' ~/~~~~/echo-hname.yaml
    ```
    
3. 변경 확인
    
    ```bash
    cat ~/~~~/echo-hname.yaml | grep replicas
    replicas: 6
    ```
    
4. 변경 내용 적용
    
    ```bash
    kubectl create -f ~/_~~~/echo-hname.yaml
    ```
    
    echo-hname 이 이미 존재한다는 에러 메시지가 나오면서  pod 수가 늘어나지 않음
    
    → apply로 오브젝트 생성하고 관리하는게 좋음
    

### 오브젝트 생성 명령어 비교

| 구분 | Run | Create | Apply |
| --- | --- | --- | --- |
| 명령 실행 | 제한적임 | 가능함 | 안 됨 |
| 파일 실행 | 안 됨 | 가능함 | 가능함 |
| 변경 가능 | 안 됨 | 안 됨 | 가능함 |
| 실행 편의성 | 매우 좋음 | 매우 좋음 | 좋음 |
| 기능 유지 | 제한적임 | 지원됨 | 다양하게 지원됨 |

위에서 본대로 create는 파일의 변경 사항을 바로 적용할 수 없다. 이럴 경우를 위해 apply로 오브젝트를 관리할 수 있다.

수정한 yaml파일에 apply 명령으로 적용

```bash
kubectl apply -f ~/~~~/echo-hname.yaml
```

오브젝트를 처음부터 apply로 생성한 것이 아니라 경고가 뜨긴 하나 작동은 함.

변경 사항이 발생할 경우를 대비해서 처음부터 apply로 생성하는 것이 좋음

ad-hoc(일회성 사용)으로 오브젝트를 생성할 때는 create, 변경이 생길 가능성이 있는 복잡한 오브젝트는 파일로 작성 후 apply로 적용하는 것이 좋다.

### pod의 컨테이너 자동 복구(self-healing)

1. pod 접속을 위해 IP를 확인(kubectl get pods -o wide)
2. exec를 통해 pod 컨테이너의 셸에 접속
    
    i : stdin(standard input)
    
    t : tty(teletypewriter, 명령줄 인터페이스)
    
    it : 표준 입력을 명령줄 인터페이스로 작성함
    
    nginx-pod의 /bin/bash를 실행해 nginx-pod의 컨테이너에서 배시 셸에 접속
    
    ```bash
    kubectl exec -it nginx-pod -- /bin/bash
    ```
    
3. 컨테이너에서 구동하는 nginx의 PID(Process ID)를 확인
    
    nginx의 PID는 언제나 1임
    
    ```bash
    cat /run/nginx.pid
    1
    ```
    

1. 슈퍼푸티에서 m-k8s의 터미널을 1개 더 띄워 nginx-pod의 IP에서 돌아가는 웹 페이지를 1초에 한 번씩 요청하는 스크립트를 실행. curl에서 요청한 값만 받도록 —silient 옵션을 추가
    
    ```bash
    i=1; while true; do sleep 1; echo  $((i++) `curl --silent
    ```
    
2. 배시 셸에서 PID 1번을 kill
    
    ```bash
    kill 1
    ```
    
3. nginx 웹 페이지를 받아오는 스크립트가 잘 작동하는지 확인 → 자동으로 복구되는 것 확인 가능
    
    nginx.pid가 생성된 시간으로 새로 생성된 프로세스인지 확인함
    
    ```bash
    kubectl exec -it nginx-pod -- /bin/bash
    ls -l run/nginx.pid
    ```


### pod의 동작 보증 기능 -  deployment 자동 복구

pod에 문제 발생 시 자동 복구해 pod이 항상 동작하도록 보장함

1. 문제 상황 만들기 - pod 삭제
    
    ```bash
    # pod 확인
    kubectl get pods
    
    # delete
    kubectl delete pods nginx-pod
    ```
    

1. pod 동작 보증을 위한 조건 확인 - 다른 pod도 삭제해서 비교해보기 위해 삭제
    
    ```bash
    kubectl delete pods echo-hname-5dakdjjk39-2jvaj
    ```
    

1. 삭제가 잘 됐나 확인하면 기존 pod이 다시 새로 생성됨을 확인할 수 있음(AGE)
    
    ```bash
    kubectl get pods
    ```
    

nginx-pod은 디플로이먼트에 속한 pod이 아니기 때문에 현재 관리되고 있지 않아서 삭제해도 다시 생성되지 않음

반면 echo-hname은 디플로이먼트에 속한 pod이고, echo-hname에 속한 pod에서 replicas를 6개로 선언했기 때문에 그 선언한 수대로 pod 수가 유지된다. → 따라서 임의의 pod을 삭제해도 replicas가 이를 확인하고 새로운 pod을 수에 맞게 생성함

디플로이먼트로 생성하는 것이 pod 동작을 보증하기 위한 조건 중 하나임.

그럼에도 디플로이먼트의 pod을 삭제하고 싶다면 상위 디플로이먼트를 삭제해야 한다.

```bash
# deployment 삭제
kubectl delete deployment echo-hname

# pod 확인
kubectl get pods
```

### 노드 자원 보호 - cordon 기능

노드 : 쿠버네티스 스케줄러에서 pod을 할당 받고 처리

쿠버네티스에선 문제가 생길 가능성이 있는 노드임을 알리기 위해 cordon 기능을 사용해 관리함

1. pod 생성
    
    ```bash
    kubectl apply -f ~/~~~/echo-hname.yaml
    ```
    

1. scale 명령으로 배포한 pod을 늘림
    
    ```bash
    kubectl scale deployment echo-name --replicas=9
    ```
    
2. 배포된 pod 확인
    
    custom-columns : 임의로 구성할 수 있음
    
    ```bash
    kubectl get pods -o=custom-columns=NAME:.metadata.name, IP:.status.podIP, 
    STATUS:.status.phase, NODE:.spec.nodeName
    ```
    
3. scale로 pod 수를 3개로 줄임
    
    ```bash
    kubectl scale deployment echo-hname --replicas=3
    ```
    
4. 각 노드에 pod 이 1개만 남았는지 확인 - 3번
5. w3-k8s 노드에서 문제가 자주 발생해 현재 상태를 보존한다고 가정하고 → w3-k8s 노드에서 cordon 명령을 실행
    
    ```bash
    # 추가로 pod 배포 시 w3-k8s 노드로는 할당되지 않음
    kubectl cordon w3-k8s
    ```
    
6. kubectl get nodes 명령을 실행해 cordon 명령이 제대로 적용됐는지 확인
    
    ```bash
    kubectl get nodes
    # SchedulingDisable 표시가 뜨면 변경된 것임
    ```
    

1. 이 상태에서 pod 수를 9개로 늘리고 확인
    
    ```bash
    kubectl scale deployment echo-hname --replicas=9
    kubectl get pods ~~ # 1개만 w3-k8s 노드라면 정상
    ```
    

1. pod 할당되지 않게 설정했던 것 해제
    
    ```bash
    kubectl uncordon w3-k8s
    ```
    

### 노드 유지보수하기 - drain 기능

drain : 지정된 노드의 pod를 전부 다른 곳으로 이동시켜 해당 노드를 유지보수 할 수 있게 제공

1. 유지보수할 노드(w3-k8s)를 pod이 없는 상태로 만듬 
    
    → 데몬셋을 지울 수 없어 명령을 수행할 수 없다고 출력
    
    ```bash
    kubectl drain w3-k8s
    error: unable to drain node "w3-k8s", aborting command...
    
    There are pending nodes to be drained:
     w3-k8s
    error: cannot delete DaemonSet-managed Pods (use --ignore-deaemonsets to ignore)
    : kube-system/calico-node-j9pic, kube-system/kube-proxy-5ltsx
    ```
    
    → drain은 실제 pod을 옮기는 것이 아니라 노드에서 pod을 삭제하고 다른 곳에 다시 생성함
    
    → DaemonSet은 각 노드에 1개만 존재하는 pod이라 drain으로 삭제할 수 없음
    
2. ignore 와 함께 사용
    
    ```bash
    kubectl drain w3-k8s --ignore-daemonsets
    ```
    
3. 노드에 pod이 없는지 확인 + 옮긴 노드에 pod이 새로 생성돼 pod 이름, IP 부여된 것 확인
    
    ```bash
    kubectl get pods -o=custom-columns=NAME:.metadata.name, IP:.status.podIP, 
    STATUS:.status.phase, NODE:.spec.nodeName
    ```
    
4. 노드 상태를 확인해보면 해당 노드가 cordon 때와 마찬가지로 SchedulingDiabled 상태임을 확인가능
    
    ```bash
    kubectl get nodes
    ```
    
5. 유지보수가 끝났다고 생각하면 uncordon 명령으로 스케줄을 받을 수 있는 상태로 복귀
    
    ```bash
    kubectl uncordon w3-k8s
    ```

### pod 업데이트하고 복구하기

업데이트 상황 

1. 새로운 기능 추가
2. 버그 발생해 버전 업데이트
3. 업데이트 중 문제 발생해 기존 버전으로 복구

pod 업데이트하기

1. 배포할 때 배포한 정보 히스토리를 기록하는 옵션 추가
    
    —record : 배포한 정보의 히스토리 기록
    
    ```bash
    kubectl apply -f ~/경로~/rollout-nginx.yaml --record
    ```
    
    rollout-nginx.yaml
    
    ```bash
    spec:
    	spec:
    		containers:
    		- name: nginx
    			image: nginx:1.15.12 # 템플릿에서 사용할 컨테이너 이미지 및 버전 지정
    ```
    
2. record 옵션으로 기록된 히스토리는 rollout history 명령을 실행해 확인할 수 있음
    
    ```bash
    kubectl rollout history deployment rollout-nginx
    ```
    

3. 배포한 pod 정보 확인

```bash
kubectl get pods \
-o-custom-columns:.metadata.name, IP:.status.podIP, STATUS:.status.phase, NODE:.sepc.nodeName
```

4. 배포한 pod 정보 확인
    
    ```bash
    curl -I --silent 172.16.103.143 | grep Server # -I : header 정보만 출력하는 옵션
    Server: nginx/1.15.12
    ```
    
5. nginx 컨테이너 버전을 1.16.0으로 업데이트하고 정보 확인
    
    ```bash
    kubectl set image deployment rollout-nginx nginx=nginx:1.16.0 --record
    
    kubectl get pods \
    -o-custom-columns:.metadata.name, IP:.status.podIP, STATUS:.status.phase, NODE:.sepc.nodeName
    ```
    
    - 정보를 확인하면 pod 이름과 IP가 변경됨을 확인할 수 있음 → 업데이트하면서 replicas에 속한 pods를 순차적으로 한 개씩 지웠다가 생성한다.
    - 업데이트 기본값은 25%, 최소값은 1개

6. 모두 업데이트된 후 디플로이먼트 상태 확인
    
    ```bash
    kubectl rollout status deployment rollout-nginx
    ```
    
7. rollout-nginx에 적용된 명령들 확인
    
    ```bash
    kubectl rollout history deployment rollout-nginx
    
    # server version 확인
    curl -I --silent 172.16.132.10 | grep Server
    ```
    

### 업데이트 실패 시 pod 복구하기

예를 들어, 없는 버전의 image를 생성하려고 시도한다면 디플로이먼트가 배포하는 단계에서 waiting으로 더 진행되지 않음 → 결국 에러 메시지 출력

관련 문제는 describe 명령으로 자세히 확인할 수 있음

```bash
kubectl describe deployment rollout-nginx
```

→ 이런 실수를 할 수 있기 때문에 업데이트 할 때 rollout을 사용하고 —record로 기록함

1. 복구를 위해서 rollout history를 확인
    
    ```bash
    kubectl rollout history deployment rollout-nginx
    ```
    

2. 명령 실행을 취소해 마지막 단계(revision 3)에서 전 단계(revision 2)로 상태를 되돌림
    
    ```bash
    kubectl rollout undo deployment rollout-nginx
    ```
    

3. pod 상태 확인
    
    ```bash
    kubectl get pods \
    -o-custom-columns:.metadata.name, IP:.status.podIP, STATUS:.status.phase, NODE:.sepc.nodeName
    ```
    
    - revision 4가 생성되고 revision 2가 삭제됨을 확인할 수 있음
    
4. 변경이 정상적으로 적용됐는지 확인
    
    ```bash
    kubectl rollout status deployment rollout-nginx
    ```
    

### 특정 시점으로 pod 복구

—to-revision 옵션 사용

```bash
kubectl rollout undo deployment rollout-nginx --to-revision=1
```

외부 사용자가 pod을 사용하는 방법 → 쿠버네티스의 서비스를 이용

서비스를 이용하는 방법

- 노드 포트를 통한 연결
- 인그레스

### 노드 포트를 통한 연결

노드 포트를 설정해 모든 워커 노드의 특정 포트를 열고 여기로 오는 모든 요청을 노드포트로 전달

노드 포트 서비스는 해당 업무를 처리할 수 있는 pod으로 요청 전달

1. 노드 포트 서비스로 외부로 접속
    
    디플로이먼트로 pod 생성
    
    ```bash
    kuebectl create deployment np-pods --image=system4admin/echo-hname
    
    # 배포한 pod 확인
    kubectl get pods
    
    ```
    
2.  노드 포트 서비스 생성
    
    ```bash
    kubectl create -f ~/경로~/nodeport.yaml
    ```
    
    nodeport.yaml
    
    → 컨테이너에 대한 정보 없이 접속 관련 네트워크 정보가 담김(protocol, port, targetPort, nodePort), 서비스 타입을 NodePort로 지정
    
3. 노드포트 서비스로 생성한 np-svc 서비스를 확인
    
    ```bash
    kubectl get services
    ```
    
    노드포트의 포트 번호가 30000으로 지정됌 - 쿠버네티스 클러스터의 내부에서 사용하는 IP로 자동 지정
    
4. 쿠버네티스 클러스터의 워커 노드 IP를 확인
    
    ```bash
    kubectl get nodes -o wide
    ```
    

5. 호스트 노트북에 웹 브라우저로 워커 노드(30000번 포트)에 외부 접속 가능한지 확인


### 부하 분산 테스트하기

디플로이먼트로 생성된 pod 1개에 접속하고 있는 중에 pod가 3개로 증가하면 접속이 어떻게 될지 → 로드밸런싱(부하 분산)이 될 지 확인

1. 호스트 노트북의 파워셸에서 아래 명령을 실행
    
    ```bash
    $i=0; while($true)
    {
    	% { $i++; write-host -NoNewline "$i $_" }
    	(Invoke-RestMethod "http://192.168.1.101:30000")-replace '\n', " " 
    }
    ```
    
    실행되면 현재 접속한 호스트 이름이 순서대로 출력됨
    
2. 쿠버네티스 마스터 노드에서 pod을 3개로 증가시킴
    
    ```bash
    kubectl scale deployment np-pods --replicas=3
    
    # 배포된 팟 확인
    kubectl get pods
    ```
    
3. 파워셸을 확인하면 배포된 pod 3개가 돌아가면서 표시되는지 확인됨
    
    → 추가된 pod은 노드 포트의 오브젝트 스펙에 적힌 np-pods와 디플로이먼트 이름을 확인해 동일하면 같은 pod으로 간주함
    
    ```bash
    spec:
    	selector:
    		app: np-pods
    ```
    

### expose로 노드포트 서비스 생성

스펙 파일 외에 expose 명령어로 노드 포트 서비스를 생성할 수 있음

1. expose로 노드 포트 서비스 생성
    
    ```bash
    kubectl expose deployment np-pods --type=NodePort --name=np-svc-v2 --port=80
    ```
    
    type은 반드시 대소문자를 구별해 NodePort로 지정해야함
    
2. expose를 사용하면 노드포트의 포트 번호를 지정할 수 없음(오브젝트 스펙으로 생성 시 노드포트 번호가 30000번으로 지정할 수 있는 것과 다름) - 30000~32767 사이에서 임의 지정

```bash
kubectl get services
```

### 인그레스(Ingress) - 사용 목적별 연결

여러 디플로이먼트가 있을 때, 그 수만큼 노드 포트 서비스를 구동해야 할 때 인그레스를 사용

인그레스 사용을 위해 인그레스 컨트롤러가 필요한데 여기서는 NGINX 인그레스 컨트롤러를 사용해 구성해봄

목적 → 사용자가 접속하는 경로에 따라 다른 결과값을 제공

NGINX 인그레스 컨트롤러 작동 순서

1. 사용자는 노드마다 설정된 노드포트를 통해 노드포트 서비스로 접속함
2. NGINX 인그레스 컨트롤러는 사용자의 접속 경로에 따라 적합한 클러스터 IP 서비스로 경로를 제공
3. 클러스터 IP 서비스는 사용자를 해당 pod으로 연결함

** 인그레스 컨트롤러는 pod과 직접 통신할 수 없음, 노드포트 또는 로드밸런스 서비스와 연동됨

1. 테스트용으로 디플로이먼트 2개 배포(in-hanme-pod, in-ip-pod)
    
    ```bash
    kubectl create deployment in-hname-pod --image=sysnet4admin/echo-hanme
    kubectl create deployment in-ip-pod --image=sysnet4admin/echo-ip
    
    # pod 상태 확인
    kubectl get pods
    ```
    

2. NGINX 인그레스 컨트롤러 설치

```bash
kubectl apply -f ~/~~~/ingress-nginx.yaml

# pod 배포 확인
kubectl get pods -n ingress-nginx

# 인그레스 경로와 작동 정의(파일로도 정의할 수 있으므로 여기선 파일로)
kubectl apply -f ~/~~~/ingress-config.yaml
```

ingress 설정 파일 → 들어오는 주소 값과 포트에 따라 노출된 서비스를 연결

ingress-config.yaml

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
	name: ingress-nginx # 이름으로 통신할 ingress 컨트롤러 확인
	annotations:
		nginx.ingress.kubernetes.io/rewrite-target: / # /(기본주소)로 지정
spec:
	rules:
	- http:
			- path: # 기본 경로 연결 서비스와 포트
				backend:
					serviceName: hname-svc-default
					servicePort: 80
			- path: /ip # 기본 경로에 ip라는 이름의 경로 추가 - 연결되는 서비스, 포트
				backend: 
					serviceName: ip-svc
					servicePort: 80
			- path: /your-directory
				backend:
					serviceName: your-svc
					servicePort: 80
```

외부에서 주소값과 노드포트를 가지고 들어오는 것은 hanme-svc-default 서비스와 연결된 pod으로 넘기고, 외부에서 들어오는 주소 값, 노트포트와 함께 뒤에 /ip를 추가한 주소 값은 ip-svc 서비스와 연결된 pod으로 넘김

3. 인그레스 설정 파일이 제대로 등록되었는지 확인
    
    ```bash
    kubectl get ingress
    
    kubectl get ingress -o yaml # 인그레스에 적용된 내용을 yaml 형식으로 출력
    ```
    

4. 외부에서 NGINX 인그레스 컨트롤러에 접속할 수 있게 노드포트 서비스로 NGINX 인그레스 컨트롤러를 외부에 노출
    
    ```bash
    kubectl apply -f ~/~~~/imgress.yaml
    ```
    
    ingress.yaml
    
    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
    	name: nginx-ingress-controller
    	namespace: ingress-nginx
    spec:
    	ports:
    	- name: http
    		protocol: TCP
    		port: 80
    		targetPort: 80
    		nodePort: 30100
    	- name: https
    		protocol: TCP
    		port: 443
    		targetPort: 443
    		nodePort: 30101
    	selector:
    		app.kubernetes.io/name: ingress-nginx
    	type: NodePort
    ```
    

5. NGINX 인그레스 컨트롤러 확인 - 네임스페이스 지정해야 확인 가능
    
    ```bash
    kubectl get services -n ingress-nginx
    ```
    
6. deployment도 서비스로 노출함 - 외부통신을 위해 클러스터 내부에서만 사용하는 pod을 클러스터 외부로 노출
    
    ```bash
    kubectl expose deployment in-hname-pod --name=hname-svc-default --port=80,443
    kubectl expose deployment in-ip-pod --name=ip-svc --port=80,443
    
    # deployment가 서비스에 노출됐는지 확인
    kubectl get services
    ```
    

7. 쿠버네티스 클러스터의 워커 노드 IP로 브라우저 접속 
    
    경로에 따라 pod IP가 다르게 반환되는지 확인
    

### 클라우드에서 구성하는 로드밸런서

위의 방식은 모두 워커 노드의 노드 포트를 통해 노드 포트 서비스로 이동하고 이걸 다시 쿠버네티스의 pod으로 보내는 구조 → 비효율적임

로드밸런서 서비스 타입을 쿠버네티스에서 제공하므로 더 간단하게 pod을 외부에 노출하면서 부하를 분산할 수 있음

로드밸런서를 사용하려면 로드밸런서를 이미 구현해 둔 서비스업체의 도움을 받아 쿠버네티스 클러스터 외부에 구현해야 한다. 클라우드에서 제공하는 쿠버네티스를 사용하고 있다면 아래와 같이 선언만 해주면 된다.

```bash
kubectl expose deployment ex-lb --type=LoadBalancer --name=ex-svc
kubectl get services ex-svc
```

위와 같이 선언하면, 쿠버네티스 클러스터에 로드밸런서 서비스가 생성돼 외부와 통신할 수 있는 IP가 부여되어 외부와 통신도 가능하고 부하가 분산된다.

### 온프레미스에서 로드밸런서를 제공하는 MetalLB

bare metal: 운영체제가 설치되지 않은 하드웨어

MetalLB는 베어 메탈로 구성된 쿠버네티스에서도 로드밸런서를 사용할 수 있게 고안된 프로젝트다.

기존의 L2 네트워크(ARP/NDP)와 L3 네트워크(BGP)로 로드밸런서를 구현함

MetalLB 컨트롤러

1. 작동 방식(프로토콜) 정의
2. EXTERNAL-IP를 부여해 관리
3. MetalLB 스피커는 정해진 작동 방식에 따라 경로를 만들 수 있도록 네트워크 정보를 광고하고 수집해 각 pod의 경로를 제공함

--- 

1. 디플로이먼트로 2종류의 pod을 생성, pod을 3개로 늘려 노드당 1개씩 배포되게 함
    
    ```bash
    kubectl create deployment lb-hname-pods --imagesysnet4admin/echo-hname
    kubectl scale deployment lb-hname-pods --replicas=3
    kubectl create deployment lb-ip-pods --imagesysnet4admin/echo-ip
    kubectl scale deployment lb-ip-pods --replicas=3
    
    # 3개씩 총 6개가 배포되었는지 확인
    kubectl get pods
    ```
    

2. yaml파일로 MetalLB 스펙 구성
    
    ```bash
    kubectl apply -f ~/~~/metalb.yaml
    ```
    
3. 배포된 MetalLB의 pod이 5개(controller 1, speaker 4)인지 확인, IP와 상태도 확인
    
    ```bash
    kubectl get pods -n metallb0system -o wide
    ```
    
4. MetalLB 설정 - ConfigMap을 사용
    
    ```bash
    kubectl apply -f ~/~~/metalb-l2config.yaml
    ```
    
    metallb-l2config.yaml
    
    ```yaml
    apiVersion: v1
    kind: ConfigMap
    metadata:
    	namespace: metallb-system
    	name: config
    data:
    	config: |
    		address-pools:
    		- name: nginx-ip-range
    			protocol: layer2 # metallb에서 제공하는 로드밸런서의 동작 방식
    			address:  # metallb에서 제공하는 로드밸런서의 ext 주소
    			- 192.168.1.11-192.168.1.13
    ```
    
5. configmap이 생성됐는지 확인
    
    ```bash
    kubectl get configmap -n metallb-system
    kubectl get configmap -n metallb-system -o yaml
    ```
    

6. 각 디플로이먼트를 로드밸런서 서비스로 노출
    
    ```bash
    kubectl expose deployment lb-hname-pods --type=LoadBalancer --name=lb-hname-svc --port=80
    kubectl expose deployment lb-ip-pods --type=LoadBalancer --name=lb-ip-svc --port=80
    ```
    
7. 생성된 로드밸런서 서비스별로 CLUSTER-IP, EXTERNAL-IP가 잘 적용됐는지 확인
    
    ```bash
    kubectl get services
    ```
    
8. 브라우저로 접속해 배포된 pod 이름이 표시되는지 확인


### 부하에 따라 자동으로 pod 수를 조절하는 HPA

HPA(Horizontal Pod Autoscaler) 작동 구조

각 워커노드에서 계측값을 수집해서 Metrics server로 전송 

→ HPA는 메트릭 서버를 통해 계측값을 전달 받음

→ 더 많은 자원이 필요한 경우 scale 요청

1. deployment 1개 생성
2. 앞서 구성한 metalLB를 로드밸런서 서비스로 설정
3. 설정된 로드밸런서 서비스와 부여된 IP 확인
    
    ```bash
    # 1
    kubectl create deployment hpa-hname-pods --images=sysnet4admin/echo-hname
    
    #2
    kubectl expose deployment hpa-hname-pods --type=LoadBalancer --name=hpa-hname-svc --port=80
    
    #3
    kubectl get services
    ```
    

4. 메트릭 서버를 오브젝트 스펙 파일로 설치

```bash
kubectl get -f ~/~~/metrics-server.yaml
```

5. 부하 확인
    
    ```bash
    kubectl top pods
    ```
    
    현재 scale 기준 값이 설정돼 있지 않아 pod 증설 시점을 알 수 없음 
    
    → pod에 부하가 걸리기 전에 scale이 실행되게 디플로이먼트 기준 값을 기록함
    
6. deployment를 새로 배포하기 보다는 기존에 배포한 디플로이먼트 내용을 edit 명령어로 직접 수정함
    
    ```bash
    kubectl edit deployment hpa-hname-pods
    
    spec:
    	containers:
    	- image: sysnet4admin/echo-hname
    	...
    	resources:
    		requests:
    			cpu: "10m"
    		limits:
    			cpu: "50m"
    
    ```
    
    request, limits 항목 → pod마다 주어진 부하량을 결정하는 기준. m은 milliunits의 약어로 1000m은 1개의 CPU가 된다. 10m은 pod의 CPU 0.01 사용을 기준으로 pod을 증설하게 설정한 것.
    
    또한 순간적으로 한 쪽 pod로 부하가 몰릴 경우를 대비해 CPU 사용 제한을 0.05로 준다.
    
7. 일정 시간이 지난 후 kubectl top pods를 실행하면 스펙이 변경돼 새로운 pod이 생성됨을 확인할 수 있음
8. hpa-hname-pod 에 autoscale을 설정해 특정 조건이 만족되는 경우 자동으로 scale이 수행되도록 함
    
    cpu-percent: CPU 사용량이 50%를 넘게 되면 autoscale하겠다는 뜻
    
    ```bash
    kubectl autoscale deployment hpa-hname-pods --min=1 --max=30 --cpu-percent=50
    ```



### 알아두면 쓸모 있는 쿠버네티스 오브젝트

### 데몬셋(DaemonSet)

디플로이먼트의 replicas가 노드 수만큼 정해진 형태. 노드 1개당 pod 1개만 생성.

이전에 Calico 네트워크 플러그인, kube-proxy 생성에도 사용했고, MetalLB의 스피커에서도 사용함

노드의 단일 접속 지점으로 노드 외부와 통신하는 역할

→ 노드를 관리하는 pod이라면 DaemonSet으로 만드는게 가장 효율적

### ConfigMap

설정 목적으로 사용하는 오브젝트

MetalLB 설정 시 컨피그맵을 사용해봄. 

인그레스는 오브젝트가 인그레스로 설정된 반면, MetalLB는 프로젝트 타입으로 정해진 오브젝트가 없어서 범용 설정으로 사용하는 configMap을 사용한 것.

### PV, PVC

pod이 동일한 설정 값을 유지하고 관리되기 위해 공유된 볼륨으로부터 공통 설정을 가져올 수 있게 도움

다양한 볼륨 스토리지 중 pv, pvc가 있음

pv(persistent volume) : 지속적으로 사용 가능한 볼륨, 볼륨 사용을 준비

pvc(persistent volume claim) : 지속적으로 사용 가능한 볼륨 요청, 준비된 볼륨에서 일정 공간 할당

### NFS 볼륨에 PV/PVC 만들고 pod에 연결하기

1. NFC 서버를 마스터 노드에 구성
    
    ```bash
    mkdir /nfs_shared
    # NFS를 받아들인 IP 정하고 옵션을 적용해 /etc/exports에 기록
    echo '/nfs_shared 192.168.1.0/24(rw,sync,no_root_squash)1 >> /etc/exports
    ```
    
2. 해당 내용을 시스템에 적용해 NFS 서버 활성화, 다음 시작에도 자동으로 적용되도록 실행
    
    ```bash
    systemctl enable --now nfs
    ```
    
3. 오브젝트 스펙 실행해 PV 생성
    
    ```bash
    kubectl apply -f ~/~~/nfs-pv.yaml
    ```
    
    ```yaml
    kind: PersistentVolumne
    ...
    spec:
    	capacity:
    		storage: 100Mi # 쓸 수 있는 양
    	accessModes:
    		- ReadWriteMany # 여러 node가 읽고 쓸 수 있게 마운트하는 옵션
    	persistentVolumeReclaimPolicy: Retain # PV가 제거될 경우 작동 방법 - 유지함
    	nfs: # nfs 서버 연결 위치 설정
    		server: 192.168.1.10
    		path: /nfs_shared
    ```
    
4. PV 상태 확인(available)
    
    ```bash
    kubectl get pv
    ```
    
5. 오브젝트 스펙 실행해 PVC 생성
    
    ```bash
    kubectl apply -f ~~/nfs-pvc.yaml
    ```
    
    ```yaml
    kind: PersistentVolumneClaim
    ...
    spec:
    	accessModes:
    		- ReadWriteMany # 여러 node가 읽고 쓸 수 있게 마운트하는 옵션
    	resources:
    		requests:
    			storage: 10Mi # 요청 볼륨, 동적 볼륨이 아닌 경우.
    ```
    
6. PVC 상태 확인
    
    ```bash
    kubectl get pvc
    
    # pv상태 - Bound로 바뀜 확인
    kubectl get pv
    ```
    
7. 생성한 PVC 볼륨 디플로이먼트 오브젝트 스펙 배포
    
    ```bash
    kubectl apply -f ~/~~/nfs-pvc-deploy.yaml
    ```
    
    ```yaml
    spec:
    	containers: 
    	- name: audit-trail # audit-trail 이미지 가져옴 -> 요청할 때마다 접속 정보 로그로 기록함
    
    		image: sysnet4admin/adult-trail
    
    		volumeMounts: # 볼륨이 마운트될 위치 지정
    		- name: nfs-vol
    			mountPath: /audit
    		volumes: # PVC로 생성된 볼륨을 마운트 하기 위해 nfs-pvc 이름 사용
    		- name: nfs-vol
    			persistentVolumneClaim:
    				claimName: nfs-pvc
    ```
    
8. 생성된 pod 확인 후 생성한 pod 중 1개로 접속
    
    ```bash
    kubectl get pods
    
    kubectl exec -it nfs-pvc-deploy-2394219b-28ad -- /bin/bash
    ```
    
9. PVC의 마운트 상태 확인
    
    ```bash
    df -h
    ```
    
10. 외부에서 pod 접속할 수 있게 로드밸런서 서비스 생성
    
    ```bash
    kubectl expose deployment nfs-pvc-deploy --type=LoadBalancer --name=nfs-pvc-deploy-svc --port=80
    ```
    
11. 브라우저로 접속해 접속한 pod에 접속 기록 남았는지 확인
    
    ```bash
    ls /audit
    cat /audit/audit_nfs-pvc-deploy-21234sdfm.log
    ```


### NFS 볼륨을 pod에 직접 마운트하기

사용자가 관리자와 동일한 단일 시스템이라면 PV, PVC를 사용할 필요가 없음 → 단순히 볼륨을 마운트하는지 확인

1. nfs-ip.yaml apply
    
    ```yaml
    spec:
    	containers:
    	...
    	volumes:
    	- name: nfs-vol
    		nfs:
    			server: 192.168.1.10
    			path: /nfs_shared
    ```
    

2. 새로 배포된 pod 확인 후 그 중 하나로 접속, 접속한 pod에서 동일한 NFS 볼륨을 바라보고 있는지 확인
    
    ```bash
    kubectl get pods
    
    kubectl exec -it nfs-ip-1287389adf-123jf -- /bin/bash
    
    ls /audit
    ```
    
    → PV, PVC를 구성하는 주체가 관리자와 사용자로 나뉨
    
    → 나뉘어 있지 않으면 PV, PVC를 통하지 않고 바로 pod에 공유가 가능한 NFS 볼륨을 마운트할 수 있음
    

### 볼륨 용량을 제한하는 방법

크게 2가지 방법

1. PVC로 PV에 요청되는 용량을 제한
    1. 용량을 제한하는 오브젝트 스펙을 가져와 적용
        
        ```bash
        kubectl apply -f ~/~~/limits-pvc.yaml
        ```
        
        limits-pvc.yaml
        
        ```yaml
        kind: LimitRange
        metadata:
        	name: storagelimits
        spec:
        	limits:
        	- type: PersistentVolumneClaim
        		max:
        			storage: 5Mi
        		min:
        			storage: 1Mi
        ```
        
        용량 제한 설정 삭제
        
        ```yaml
        kubectl delete limitranges storagelimits
        ```
        
2. 스토리지 리소스에 대한 사용량을 제한
    1. 총 누적 사용량을 제한하기 위한 다음 오브젝트 스펙을 적용
        
        ```bash
        kubectl apply -f ~/~~~/quota-pvc.yaml
        ```
        
        quota-pvc.yaml
        
        ```yaml
        kind: ResourceQuota
        metadata:
        	name: storagequota
        spec:
        	hard:
        		persistentvolumneclaims: "5" # PVC 5개 넘지 않게 제한
        		request.storage: "25Mi" # 용량은 25Mi가 넘지 않게 제한
        ```
        
        ```bash
        kubectl delete resourcequotas storagequota
        ```
        

### 스테이트풀셋(StatefuleSet)

pod이 생성되는 이름과 순서를 예측해야할 때 - 레디스, 주키퍼, 카산드라, 몽고DB 등의 마스터-슬레이브 구조 시스템에 필요

- volumneClaimTemplates 기능을 사용해 PVC를 자동 생성 가능
    - 각 pod이 순서대로 생성되어 고정된 이름, 볼륨, 설정 등을 가질 수 있음
- 효율성면에서 좋은 구조는 아님

1. 스테이트풀셋도 yaml으로 설정
    
    ```bash
    kubectl apply -f ~~/~~~/nfs-pvc-sts.yaml
    ```
    
    nfs-pvc-sts.yaml
    
    ```yaml
    kind: StatefulSet
    ...
    spec:
    	replicas: 4
    	serviceName: sts-svc-domain # statefulset에 필요함
    	selector:
    		matchLabels:
    			app: nfs-pvc-sts
    		spec:
    			containers:
    			- name: aduit-trail
    			...
    			volumes:
    			- name: nfs-vol
    				persistentVolumeClaim:
    					claimName: nfs-pvc
    ```
    

2. 명령 실행 후 순서대로 하나씩 생성되는지 확인
    
    ```bash
    kubectl get pods -w
    ```
    
    statefulset은 expose 명령어를 지원하지 않아 파일로 로드밸런서 서비스를 작성해 실행해야 함
    
    (expose 명령으로 서비스 생성할 수 있는 오브젝트 - 디플로이먼트, 파드, 레플리카셋, 레플리케이션 컨트롤러)
    
3. 스테이트풀셋을 노출하기 위한 서비스 생성, 로드밸런서 서비스 확인\
    
    ```bash
    kubectl apply -f ~/~~~/nfs-pvc-sts-svc.yaml
    
    kubectl get services
    ```
    

**statefulset은 헤드리스 서비스로 노출하나요?**

일반적으로는 맞는 얘기. 헤드리스 서비스는 IP를 가지지 않는 서비스 타입으로 중요 자원인 IP를 절약할 수 있고, 스테이트풀셋 같은 상태를 가진 오브젝트를 모두 노출하지 않고 상태 값을 외부에 알리고 싶은 것만 선택적으로 노출할 수 있다. 

일반적으로는 스테이트풀셋은 헤드리스 서비스로 노출하나 고정 이름을 사용해서 외부에 모든 스테이트풀셋을 노출하고자 할 때는 노드포트나 로드밸런서 서비스로 노출할 수 있다.

IP를 할당하지 않은 상태로 각 pod의 이름과 노출된 서비스 이름 등을 조합한 도메인 이름으로 쿠버네티스 클러스터 내에서 통신할 수 있는 상태로 만들 수 있음(CoreDNS가 이를 가능하게 함)

 

1. 스테이트풀셋 삭제
    
    ```bash
    kubectl delete statefulset nfs-pvc-sts
    ```
    
    일반적으로 스테이트풀셋은 volumeClaimTemplates을 이용해 자동으로 각 pod에 독립적인 스토리지를 할당해 구성할 수 있다.